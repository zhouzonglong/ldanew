自然语言处理：
1、分词
    常用：基于马尔科夫理论 后一个词只与前一个词有关
              P(Ai1,Ai2,...,Aini)=P(Ai1)P(Ai2|Ai1)P(Ai3|Ai2)...P(Aini|Ai(ni−1))
          n元模型：后一个词与前n个词有关
              P(Ai1,Ai2,...,Aini)=P(Ai1)P(Ai2|Ai1)P(Ai3|Ai1，Ai2)...P(Aini|Ai(ni−2)，Ai(ni−1))
          维比特算法：计算最短路径（一个句子）
    文本挖掘一般工具：简单的英文分词不需要任何工具，通过空格和标点符号就可以分词了，而进一步的英文分词推荐使用nltk。
    对于中文分词，则推荐用结巴分词（jieba）。
2、向量化
    词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。
    但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。

3、特征降维 hash trick/特征向量化
    和PCA类似，Hash Trick降维后的特征我们已经不知道它代表的特征名字和意义。
    此时我们不能像上一节向量化时候可以知道每一列的意义，所以Hash Trick的解释性不强。

    这里我们对向量化与它的特例Hash Trick做一个总结。在特征预处理的时候，我们什么时候用一般意义的向量化，什么时候用Hash Trick呢？标准也很简单。
        一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。
        而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。
        因此，实际工作中我使用的都是特征向量化。
4、TFIDF
    TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，
    我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。


5、主题模型
    通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。
    SVD分解后，Uil对应第i个文本和第l个主题的相关度。Vjm对应第j个词和第m个词义的相关度。Σlm对应第l个主题和第m个词义的相关度。
    Am×n≈Um×kΣk×kVTk×n

    LSI可以用于文本相似度计算
    sim(d1,d2)=(−0.4945)∗(−0.6458)+(0.6492)∗(−0.7194)/√(−0.4945)2+0.64922*√(−0.6458)2+(−0.7194)2

    总结svd：
    主要的问题有：

　　　　1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。
　　　　2） 主题值的选取对结果的影响非常大，很难选择合适的k值。
　　　　3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。

　　　　对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。
        对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。
        对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。

　　　　回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。

        -------------------------------------------------------NMF------------------------------------------------------------------------
        非负矩阵分解(NMF)是一种非常常用的矩阵分解方法，它可以适用于很多领域，比如图像特征识、语音识别、文本主题模型。
            Am×n≈Wm×kHk×n
        和LSI相比，我们不光得到了文本和主题的关系，还得到了直观的概率解释，同时分解速度也不错。当然NMF由于是两个矩阵，相比LSI的三矩阵，NMF不能解决词和词义的相关度问题。这是一个小小的代价。

        NMF总结：
        NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，而对不在样本中的文本是无法识别其主题的。根本原因在于NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型。而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。

6、朴素贝叶斯的主要优点有：

　　　　1）朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。

　　　　2）对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。

　　　　3）对缺失数据不太敏感，算法也比较简单，常用于文本分类。

　　　　朴素贝叶斯的主要缺点有：　　　

　　　　1） 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，
            在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

　　　　2）需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。

　　　　3）由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。

　　　　4）对输入数据的表达形式很敏感。

交叉验证：如果数据样本量小于一万条，我们就会采用交叉验证来训练优化选择模型
        N小于50时，我一般采用留一交叉验证。

